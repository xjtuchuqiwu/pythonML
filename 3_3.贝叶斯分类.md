<h2>贝叶斯分类</h2>

<!-- TOC -->

- [一:机器学习基本概念](#一机器学习基本概念)
- [二.贝叶斯定理](#二贝叶斯定理)
    - [1.基本概念](#1基本概念)
    - [2.贝叶斯公式](#2贝叶斯公式)
    - [3.高斯分布和大数定律](#3高斯分布和大数定律)
- [三.贝叶斯分类](#三贝叶斯分类)
    - [1.贝叶斯分类原理](#1贝叶斯分类原理)
    - [2.三种常见的模型](#2三种常见的模型)
        - [(1)多项式模型](#1多项式模型)
        - [(2)高斯模型](#2高斯模型)
        - [(3)伯努利模型](#3伯努利模型)
    - [3.朴素贝叶斯的优缺点](#3朴素贝叶斯的优缺点)
    - [4.sklearn构造朴素贝叶斯](#4sklearn构造朴素贝叶斯)
    - [(1) GaussianNB类](#1-gaussiannb类)
    - [(2) MultinomialNB类](#2-multinomialnb类)
    - [(3) BernoulliNB类](#3-bernoullinb类)
- [四.朴素贝叶斯的实现](#四朴素贝叶斯的实现)
    - [1.python sklearn](#1python-sklearn)
    - [2.sparkml](#2sparkml)

<!-- /TOC -->

#### 一:机器学习基本概念
*   R、Sas不能处理大数据
*   机器学习的算法是不区分行业的，区分的只是不同的应用场景，不同的数据量使用机器学习的算法会花费不同的时间
*   当数据规模非常大的时候，如何应用机器学习，有几种方式:
    *  第一种方式是将大规模的数据变为小规模，利用抽样，降低数据量
    *  第二种方式是，采用分布式计算的方式进行训练，例如，使用sparkml分布式计算



#### 二.贝叶斯定理
参考 [深入浅出贝叶斯](https://www.cnblogs.com/zhoulujun/p/8893393.html)

##### 1.基本概念
在引出贝叶斯定理之前，先学习几个定义：
*   边缘概率
>边缘概率（又称先验概率）：某个事件发生的概率。边缘概率是这样得到的：在联合概率中，把最终结果中那些不需要的事件通过合并成它们的全概率，而消去它们（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率），这称为边缘化（marginalization），比如A的边缘概率表示为P(A)，B的边缘概率表示为P(B)
*   联合概率
>联合概率表示两个事件共同发生的概率。A与B的联合概率表示为P(A∩B)或者P(A,B)。
*   条件概率
>条件概率（又称后验概率）：事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P(A|B)，读作“在B条件下A的概率”,。

```java
首先，事件B发生之前，我们对事件A的发生有一个基本的概率判断，称为A的先验概率，用P(A)表示；
其次，事件B发生之后，我们对事件A的发生概率重新评估，称为A的后验概率，用P(A|B)表示；
类似的，事件A发生之前，我们对事件B的发生有一个基本的概率判断，称为B的先验概率，用P(B)表示；
同样，事件A发生之后，我们对事件B的发生概率重新评估，称为B的后验概率，用P(B|A)表示
```
*   似然
>似然就是从观察值来推测参数，例如：抛一枚硬币十次，观察到的结果十次都是朝上，那么请问“这个硬币是正反两面均匀的可能性是多少”，这里求的“可能性”就是似然


##### 2.贝叶斯公式
条件概率公式：
$$P(A | B)=\frac{P(A B)}{P(B)}$$

贝叶斯公式：
$$ P\left(B_{i} | A\right)=\frac{P\left(B_{i}\right) P\left(A | B_{i}\right)}{\sum_{j=1}^{n} P\left(B_{j}\right) P\left(A | B_{j}\right)} $$

(分母是全概率公式,分子使用了假设独立同分布)

##### 3.高斯分布和大数定律
当特征是连续变量的时候，假设特征分布为高斯分布(正太分布)，根据样本算出均值和方差，再求得概率
$$P\left(x_{i} | y\right)=\frac{1}{\sqrt{2 \pi \sigma_{y}^{2}}} \exp \left(-\frac{\left(x_{i}-\mu_{y}\right)^{2}}{2 \sigma_{y}^{2}}\right)$$
连续值可以利用正态分布的密度函数求出事件发生的概率
所以，贝叶斯中，既可以有离散值也可以有连续值   
<font size=2> (那么如何构造数据集更好呢?使用离散还是连续呢？)
</font>
#### 三.贝叶斯分类
#####  1.贝叶斯分类原理
[浅显易懂的解释](https://blog.csdn.net/a925907195/article/details/53822830)   讲的非常好，一文看懂
```java
总结:
1.分类问题往往采用的是经验性方法，贝叶斯其实就是很好的利用先前经验做出预测的方法，在同样的样本下，
如果出现了截然不同的结果，这个时候该相信谁，就是根据贝叶斯公式，看哪种情况的概率更大，就选择相信谁
2。贝叶斯分类中的分子需要满足独立同分布，也就是说在构造数据集的时候，特征的选择需要独立同分布
3.步骤(二分类)
(1)先计算数据集中，每个分类结果出现的概率，例如p(y=0)=0.5,p(y=1)=0.5
(2)然后计算每个类别条件下，各个特征属性出现的概率，例如:p(a<0.5|y=0)=0.1,p(a>0.5|y=1)=0.3,...每个特征根据数据集都算出来
(3)然后根据贝叶斯公式计算全部特征属性所构造出的分类概率，选择最大的那个，作为分类
(4)Laplace校准:防止出现 P(a|y)=0 的情况，具体就是，对每个类别下所有划分的计数加1，在数据规模比较大的情况下，这个1没有啥影响，并且避免了计数为0的情况
```

#####  2.三种常见的模型
###### (1)多项式模型
*   多项式模型在计算先验概率P(Yk)和条件概率P(xi|Yk)时，会做一些平滑处理
$P\left(Y_{k}\right)=\frac{N_{Y_{k}}+\alpha}{N+K \alpha}$ , N：样本数 ,NYk：类别为Yk的样本数 ,K：总的类别个数 ,α：平滑值
&nbsp;
    
    $P\left(x_{i} | Y_{k}\right)=\frac{N_{Y_{k}, x_{i}}+\alpha}{N_{Y_{k}}+n \alpha}$  ,NYk,xi ：类别为Yk，且特征为x1的样本数 ,n：特征x1可以选择的数量


###### (2)高斯模型
*   高斯模型就是上面所说的处理连续值的情况

###### (3)伯努利模型 
*   伯努利模型适用于离散特征的情况，伯努利模型中每个特征的取值只能是1和0。

*   $P\left(x_{i} | y\right)=P(i | y) x_{i}+(1-P(i | y))\left(1-x_{i}\right)$



#####  3.朴素贝叶斯的优缺点
优点
```java
(1).发源于古典数学理论，有稳定的分类效率
(2).对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。
(3)对缺失数据不太敏感，算法也比较简单，常用于文本分类
```
缺点
```java
(1)朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。
而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。
(2)需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种(比如，高斯，伯努利，多项式)，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳
(3)对输入数据的表达形式很敏感
```
>尽管朴素贝叶斯（Naïve Bayes）被认为是一个不错的分类器，但它被认为是一个不好的估计器，所以一般不用它来获得测试向量的概率估计。


#####  4.sklearn构造朴素贝叶斯

*   和其他机器学习算法相比，sklearn中朴素贝叶斯算法的可调参数相对较少
*   在scikit-learn中，一共有3个朴素贝叶斯的分类算法类。分别是GaussianNB，MultinomialNB和BernoulliNB。其中GaussianNB就是先验为高斯分布的朴素贝叶斯，MultinomialNB就是先验为多项式分布的朴素贝叶斯，而BernoulliNB就是先验为伯努利分布的朴素贝叶斯。
*   一般来说，如果样本特征的分布大部分是连续值，使用GaussianNB会比较好。如果如果样本特征的大部分是多元离散值，使用MultinomialNB比较合适。而如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用BernoulliNB。

##### (1) GaussianNB类
*   GaussianNB类的主要参数仅有一个，即先验概率priors ,对应Y的各个类别的先验概率P(Y=Ck), 这个值默认不给出，如果不给出此时P(Y=Ck)=mk/m。其中m为训练集样本总数量，mk为输出为第k类别的训练集样本数。如果给出的话就以priors 为准。
*  在使用GaussianNB的fit方法拟合数据后，我们可以进行预测。此时预测有三种方法，包括predict，predict_log_proba和predict_proba。
    *   predict方法就是我们最常用的预测方法，直接给出测试集的预测类别输出。
    *   predict_proba会给出测试集样本在各个类别上预测的概率
    *   predict_log_proba会给出测试集样本在各个类别上预测的概率的一个对数转化
*   GaussianNB一个重要的功能是有 partial_fit方法，这个方法的一般用在如果训练集数据量非常大，一次不能全部载入内存的时候。这时我们可以把训练集分成若干等分，重复调用partial_fit来一步步的学习训练集，非常方便
```python
import numpy as np
from sklearn.naive_bayes import GaussianNB
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
Y = np.array([1, 1, 1, 2, 2, 2])
clf = GaussianNB()          
#拟合数据
clf.fit(X, Y)                          #训练
print "==Predict result by predict=="
print(clf.predict([[-0.8, -1]]))
print "==Predict result by predict_proba=="
print(clf.predict_proba([[-0.8, -1]]))
print "==Predict result by predict_log_proba=="
print(clf.predict_log_proba([[-0.8, -1]]))
```


##### (2) MultinomialNB类
 $P\left(X_{j}=x_{j l} | Y=C_{k}\right)=\frac{x_{j l}+\lambda}{m_{k}+n \lambda}$
*   MultinomialNB假设特征的先验概率为多项式分布,如上
*   MultinomialNB参数比GaussianNB多，但是一共也只有仅仅3个 
    *   参数alpha即为上面的常数λ，如果你没有特别的需要，用默认的1即可。如果发现拟合的不好，需要调优时，可以选择稍大于1或者稍小于1的数。
    *   布尔参数fit_prior表示是否要考虑先验概率，如果是false,则所有的样本类别输出都有相同的类别先验概率
    *   否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率，此时的先验概率为P(Y=Ck)=mk/m。其中m为训练集样本总数量，mk为输出为第k类别的训练集样本数

*   样本特征的大部分是多元离散值，使用MultinomialNB比较合适,如，0，1，2，3
*   训练后的预测方法和GaussianNB一样


##### (3) BernoulliNB类
$P\left(X_{j}=x_{j l} | Y=C_{k}\right)=P\left(j | Y=C_{k}\right) x_{j l}+\left(1-P\left(j | Y=C_{k}\right)\left(1-x_{j l}\right)\right.$
*   BernoulliNB假设特征的先验概率为二元伯努利分布
*   $\mathcal{X} j l$只能取值0或者1
*   BernoulliNB一共有4个参数,其中3个参数的名字和意义和MultinomialNB完全相同。
唯一增加的一个参数是binarize。这个参数主要是用来帮BernoulliNB处理二项分布的，可以是数值或者不输入。如果不输入，则BernoulliNB认为每个数据特征都已经是二元的。否则的话，小于binarize的会归为一类，大于binarize的会归为另外一类.

#### 四.朴素贝叶斯的实现
##### 1.python sklearn 

##### 2.sparkml
示例:
```java
import org.apache.spark.ml.classification.NaiveBayes
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

// Load the data stored in LIBSVM format as a DataFrame.
val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

// Split the data into training and test sets (30% held out for testing)
val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3), seed = 1234L)

// Train a NaiveBayes model.
val model = new NaiveBayes()
  .fit(trainingData)           //lambda = 1.0, modelType = "multinomial" smoothing=1.0

// Select example rows to display.
val predictions = model.transform(testData)
predictions.show()

// Select (prediction, true label) and compute test error
val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("label")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val accuracy = evaluator.evaluate(predictions)
println("Test set accuracy = " + accuracy)
```



























